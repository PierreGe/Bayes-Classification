\documentclass[a4paper,10pt]{article}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[left=2.5cm,top=2cm,right=2.5cm,nohead,nofoot]{geometry}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{amsmath}

\linespread{1.1}



\begin{document}

\begin{titlepage}
\begin{center}
\textbf{\textsc{UNIVERSIT\'E DE MONTR\'EAL}}\\
%\textbf{\textsc{Faculté des Sciences}}\\
%\textbf{\textsc{Département d'Informatique}}
\vfill{}\vfill{}
\begin{center}{\Huge Rapport : Devoir1 }\end{center}{\Huge \par}
\begin{center}{\large Pierre Gérard \\ Mathieu Bouchard}\end{center}{\Huge \par}
\vfill{}\vfill{} \vfill{}
\begin{center}{\large \textbf{IFT3395-6390 Fondements de l'apprentissage machine}}\hfill{\\Pascal Vincent, Alexandre de Brébisson et César Laurent}\end{center}{\large\par}
\vfill{}\vfill{}\enlargethispage{3cm}
\textbf{Année académique 2015~-~2016}
\end{center}
\end{titlepage}

%\begin{abstract}
%Ce rapport présente ...
%\end{abstract}


\tableofcontents

\pagebreak

\section{Partie Théorique : Estimation de densité}

\subsection{Densité avec des fenêtres de Parzen a noyau Gaussien isotropique}

a) $\mathcal{N}_{x^{(i)}, \sigma^{2}} = \frac{1}{(2 \pi)^{d/2} \sigma^{d}} e^{ \frac{-1}{2} \frac{d(x^{i},x)^{2}}{\sigma^{2}} } $



b) Si on choisit de prendre tous les points du voisinages
$ p(x) =  \frac{1}{n} \sum_{i=1}^{n} \mathcal{N}_{x^{(i)}, \sigma^{2}} $

c) Hyper-paramètre: $\sigma \in \mathds{R}$

Paramètre : On mémorise l'ensemble des données d'entrainement. $( n \times \mathds{R}^{d})$

On considère cette méthode comme non-paramétrique car le nombre de paramètre varie avec la taille de l'ensemble de données.



d)
Cette densité de probabilité est toujours positive. En effet, l'ensemble de paramètre est positif et on les multiplie et additionne. 

L'intégral étant compliqué à calculer. Nous avons testé qu'elle somme à de manière empirique en calculant 2000 p(x) different sur l'interval de -10 à 10.

\begin{verbatim}
def parzenEqualToOne(self):
        sigma = 0.349
        dg = densite_fonction.DensiteParzen(1, sigma)
        dg.train(self.oneDimData)
        input = [ i/100. for i in range(-1000,1000)]
        fig = plt.figure()
        ax = fig.add_subplot(111)
        n, bins, rectangles = ax.hist(input, 50, normed=True)
        print("Resultat = " + str(numpy.sum(n * numpy.diff(bins))))
\end{verbatim}

Ce code donne $Resultat = 1.0$

\subsection{Densité paramétrique avec Gaussienne isotropique}

a) $p(x) = \frac{1}{(2 \pi)^{d/2} \sigma^{d}} e^{\frac{-\mid\mid x- \mu \mid\mid ^{2}}{2 \sigma^{2}} }$


$\mu$ est le est le vecteur des moyennes, de dimension d.
$\Sigma$ est la matrice de covariance, de dimension 1.



b) Dans le cas de l'estimation par fenêtre de Parzen, il nous faut effectuer une moyenne sur n densités Gaussiennes et, pour chacune d'elles, le point x(i) sert de moyenne $\mathcal{N}_{x^{(i)}, \sigma^{2}}$ avec $\sigma$ qui est un hyperparamètre. Dans le cas présent, on utilise une seule densité Gaussienne pour laquelle il faudra apprendre les valeurs de moyenne $\mu$ et d'écart-type $\sigma$.

c)

Paramètres :
\begin{itemize}
	\item $\mu$ est le est le vecteur des moyennes, de dimension d.
	\item $\Sigma$ est la matrice de covariance, de dimension 1.
\end{itemize}

Pas d'hyper-paramètre.


\subsection{Densité paramétrique avec Gaussienne diagonale}

a) $p(x) = \frac{1}{(2 \pi)^{d/2} \sqrt{\mid{\Sigma} \mid}} e^{\frac{-1}{2} (x- \mu)^{T} \Sigma^{-1} (x- \mu) }$
\begin{itemize}
	\item $\mu$ est le est le vecteur des moyennes, de dimension d.
	\item $\Sigma$ est la matrice de covariance, de dimension d.
\end{itemize}

\[
\Sigma =
\begin{bmatrix}
    \sigma_{1} & 0 & 0 & \dots  & 0 \\
    0 & \sigma_{2} & 0 & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & \sigma_{n}
\end{bmatrix}
\]

b)Indépendance des composantes d'un vecteur aléatoire suivant une distribution Gaussienne diagonale :

Soit $x \in \mathds{R}^{d}$ un vecteur aléatoire qui suit une distribution Gaussienne diagonale $\mathcal{N}_{\mu , \sigma(x)}$

Posons $x - \mu = k$ et $\frac{1}{(2 \pi)^{d/2} \sqrt{\mid{\Sigma} \mid}} = s$
\[
\Sigma =
\begin{bmatrix}
    \sigma_{1} & 0 & 0 & \dots  & 0 \\
    0 & \sigma_{2} & 0 & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & \sigma_{d}
\end{bmatrix}
\Sigma^{-1} =
\begin{bmatrix}
    \frac{1}{\sigma_{1}} & 0 & 0 & \dots  & 0 \\
    0 & \frac{1}{\sigma_{2}} & 0 & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & \frac{1}{\sigma_{d}}
\end{bmatrix}
\]

$p(x) = \mathcal{N}_{\mu , \Sigma(x)} = \frac{1}{(2 \pi)^{d/2} \sqrt{\mid{\Sigma} \mid}} e^{\frac{-1}{2} (x- \mu)^{T} \Sigma^{-1} (x- \mu) } = s \times e^{\frac{-1}{2} k^{t} \Sigma^{-1} k }$

$p(x) = s \times e^{\frac{-1}{2} (\frac{k_{1}}{\sigma_{1}^{2}} \frac{k_{2}}{\sigma_{2}^{2}} ... \frac{k_{d}}{\sigma_{d}^{2}} )k }  = s \times e^{\frac{-1}{2} \sum_{i=1}^{d} \frac{k_{i}^{2} }{\sigma_{i}^{2}} }$

$p(x) = s \prod_{i=1}^{d} e^{\frac{-1}{2} \frac{k_{i}^{2} }{\sigma_{i}^{2}} }  = s \prod_{i=1}^{d} e^{\frac{-1}{2} (x^{i} - \mu)^{T} \Sigma^{-1} (x^{i} - \mu) } $ où $x^{i}$ est un vecteur de taille $d$ avec la $i^{eme}$ valeur de $x$ en position $i$ et des 0 partout ailleurs.

$p(x) =\frac{1}{(2 \pi)^{d/2} \sqrt{\mid{\Sigma} \mid}} \prod_{i=1}^{d} e^{\frac{-1}{2} (x^{i} - \mu)^{T} \Sigma^{-1} (x^{i} - \mu) }  $

$p(x) = \prod_{i=1}^{d} p(x^{i}) $


La densité estimée pour à partir d'un vecteur aléatoire $x \in R$ qui suit une distribution gaussienne diagonale est égale au produit des densités estimées des composantes de ce vecteur aléatoire. On peut en conclure que ces composantes sont des variables aléatoires indépendantes.


c) $ J(\theta) =\widehat{R}(f,D) = \frac{1}{n} \sum_{i=1}^{n} -log (p(x^{i}))$

On cherche donc $\theta^{*}$ tel que $\theta^{*} = argmin_{\theta} \widehat{R}(f,D)$

$ \theta = ( \mu, \sigma )$

d) On calcul le gradient $ \triangledown J(\theta) = $

$\widehat{R}(f,D) = \frac{1}{n} \sum_{i=1}^{n} -log (\frac{1}{(2 \pi)^{d/2} \sqrt{\mid{\Sigma} \mid}} e^{\frac{-1}{2} (x- \mu)^{T} \Sigma^{-1} (x- \mu) })$

$ \widehat{R}(f,D) = \frac{1}{n} \sum_{i=1}^{n} - ( -log((2 \pi)^{d/2} \sqrt{\mid{\Sigma} \mid} ) + (\frac{-1}{2} (x- \mu)^{T} \Sigma^{-1} (x- \mu) ) )$

$ \widehat{R}(f,D) = \frac{1}{n} \sum_{i=1}^{n} log((2 \pi)^{d/2} )+log(\sqrt{\mid{\Sigma} \mid} ) - (\frac{-1}{2} (x- \mu)^{T} \Sigma^{-1} (x- \mu) ) $

Pour une matrice diagonale, le déterminant vaut le produit des éléments sur la diagonale.

$\sqrt{\mid{\Sigma} \mid} = \sqrt{ \prod_{k=1}^{d} \sigma^{2}_{k} } =   \prod_{k=1}^{d} \sqrt{\sigma^{2}_{k} } = \prod_{k=1}^{d} \sigma_{k} $ car $\sigma$ positif

$ \widehat{R}(f,D) = \frac{1}{n} \sum_{i=1}^{n} log((2 \pi)^{d/2} )+log(\prod_{k=1}^{d} \sigma_{k} ) - (\frac{-1}{2} (x- \mu)^{T} \Sigma^{-1} (x- \mu) ) $

Dérivé partielle pour $\mu$

$\frac{\partial J}{\partial \mu} = 0 + 0 + \frac{\partial }{\partial \mu} \sum_{i=1}^{n}  - (\frac{-1}{2n} (x- \mu)^{T} \Sigma^{-1} (x- \mu) )$ 

\subsection{Choix du modèle}

La première tâche à effectuer pour choisir le modèle le plus approprié est de sélectionner un ensemble de tous les modèles que nous pourrions effectuer. Ici, il s'agit des approches présentées aux sections 1.1, 1.2 et 1.3. Ensuite, pour chacun des modèles sélectionnées, on procède ainsi :

\begin{enumerate}
	\item Soit $A$ , le modèle courant.
	\item Si le modèle contient des hyper-paramètres, répéter la procédure pour toutes les valeurs acceptables que ces hyper-paramètres peuvent prendre. Appelons cette configuration $\lambda$. Rappelons que l'approche des fenêtres de Parzen à noyau isotropique contient un hyper- paramètre (l'écart-type $\sigma$) tandis que les approches 2 et 3 n'en n'ont aucun ( $\lambda= \O$ ).
	\item Mélanger les éléments de l'ensemble $D$ s'ils étaient ordonnés.
	\item Séparer l'ensemble d'entraînement $D$ en deux sous-ensembles $D_{train}$ et $D_{valid}$. L'ensemble d’entraînement devrait être plus grand que l'ensemble de validation. $x \in \mathds{R}$ qui suit une distribution Gaussienne
	\item Entraîner $A$ avec la configuration d'hyper-paramètres choisis sur l'ensemble d'entraînement. On obtient alors : $\widehat{f} (A_{\lambda})=A_{\lambda}(D_{train})$
	\item Evaluer le résultat à l'aide de l'ensemble de validation. $e_{(A_{\lambda})} = \widehat{R} ( \widehat{f}_{(A_{\lambda})} , D_{valid} )$
\end{enumerate}

Une fois que ce travail a été effectuer pour tous les algorithmes et, pour chacun, sur chaque configurations d'hyper-paramètres $\lambda$ jugés admissibles, on regarde les valeurs de $e_{(A_{\lambda})}$ obtenues. Le modèle $A_{\lambda}$ qui retourne une erreur de validation minimale $e*_{A_{\lambda}}$ sera alors sélectionner. 
Nous aurions également pu séparer $D$ en 3 sous-ensembles plutôt que 2 et ainsi ajouter un ensemble de test. Une fois l'algorithme optimal trouvé à l'aide de l'ensemble d'entraînement et de l'ensemble de validation, nous aurions pu calculer un estimé non-biaisé de la performance de généralisation de l'algorithme choisit en calculant l'erreur empirique de validation sur l'ensemble de test; soit un ensemble de données dont le valeurs n'ont jamais été utilisées pour sélectionner l'algorithme dans les étapes précédantes. Cette étape nous permet de nous assurer (si nous avons assez de donner pour pouvoir effectuer un test concluent) que l'algorithme sélectionné est capable de généraliser sur de nouvelles données et qu'il n'est pas juste efficace sur les données de $D_{train}$ et $D_{valid}$.



\section{Partie Théorique : Classifieurs de Bayes}

\subsection{Classifieur de Bayes obtenu avec des densités paramétriques Gaussiennes diagonales}

a)Pour la probabilité a postériori.
\begin{itemize}
	\item $\mu$ est le est le vecteur des moyennes, de dimension d.
	\item $\Sigma$ est la matrice de covariance, de dimension d.
\end{itemize}
Pour la probabilité a priori.
\begin{itemize}
	\item La proportion d'element de chaque classe, de dimension m.
\end{itemize}

b)

c)
$P(Y=c | X=x)  = \frac{P(X=x | Y=c) P(Y=c)}{P(X=x)} $

$P(Y=c | X=x)  = \frac{ \hat{p}_{c}(x) \hat{P}_{c}}{ \sum_{c'=1}^{m} \hat{p}_{c'} \hat{P}_{c'} } $

avec $ \hat{p}_{c}(x) $ la densité de probabilité apprise (1.2 a) sur tout les points de la classe c.

\subsection{Classifieur de Bayes obtenu avec des fenêtres de Parzen à noyau Gaussien isotropique}

a) Pour la probabilité à postériori
\begin{itemize}
	\item L'ensemble des données d'entrainement.
\end{itemize}

Pour la probabilité a priori.
\begin{itemize}
	\item La proportion d'element de chaque classe, de dimension m.
\end{itemize}

b) $P(Y=c | X=x)  = \frac{P(X=x | Y=c) P(Y=c)}{P(X=x)} $

$P(Y=c | X=x)  = \frac{ \hat{p}_{c}(x) \hat{P}_{c}}{ \sum_{c'=1}^{m} \hat{p}_{c'} \hat{P}_{c'} } $

avec $ \hat{p}_{c}(x) $ la densité de probabilité apprise (1.3 a) sur tout les points de la classe c.


\subsection{Classifieur de Parzen obtenu avec des fenêtres de Parzen à noyau Gaussien isotropique}

a)

$P(Y=c | X=x)  = \frac{ \hat{p}_{c}(x) }{ \sum_{c'=1}^{m} \hat{p}_{c'} } $

avec $ \hat{p}_{c}(x) =  \frac{1}{ \sum_{i=1}^{n} K(x_{i}, x) } \sum_{i=1}^{n} K(x_{i}, x) onehot_{m}(Y_{i}) $

et avec $ K(x_{i}, x) = \frac{1}{ (2 \pi)^{d/2} \sigma^{d}}  e^{ \frac{-1}{2} \frac{d(x^{i},x)^{2}}{\sigma^{2}}} $ 


b)

\section{Implémentation}

Pour tester notre code et l'implémentation, exécuter simplement dans le dossier $src$ :
\begin{verbatim}
	python main.py
\end{verbatim}
L'ensemble des graphiques vont être sauvé comme image.

Note : Les modules suivants utilisé en labo sont nécessaire : numpy, matplotlib, pylab.


\section{Partie Pratique : Estimation de densité}

\subsection{Densité 1D}

\begin{figure}[H]
	\includegraphics[width=12cm]{images/1D_Gaussian_and_Parzen_probability_density.png} 
	\centering
	\caption{Comparaison de la densité paramétrique et la densité de Parzen avec différents sigma}
	\label{fig:comp}
\end{figure}

\subsection{Densité 2D}

\subsubsection{Densité paramétrique gaussienne}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/2D_Gaussian_border.png} 
	\centering
	\caption{Densité paramétrique gaussienne}
	\label{fig:comp}
\end{figure}

\subsubsection{Densité de Parzen : sigma trop petit}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/2D_Parzen_sigma_petit.png} 
	\centering
	\caption{Densité de Parzen : sigma trop petit}
	\label{fig:comp}
\end{figure}

\subsubsection{Densité de Parzen : sigma trop grand}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/2D_Parzen_sigma_grand.png} 
	\centering
	\caption{Densité de Parzen : sigma trop grand}
	\label{fig:comp}
\end{figure}

\subsubsection{Densité de Parzen : sigma adéquat}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/2D_Parzen_sigma_ad.png} 
	\centering
	\caption{Densité de Parzen : sigma adéquat}
	\label{fig:comp}
\end{figure}

\section{Partie Pratique : Classifieur de Bayes}

\subsection{Avec densités paramétriques gaussienne diagonales}

\subsubsection{Graphique pour d=2}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/bayes_gaussienne.png} 
	\centering
	\caption{Graphique pour d=2}
	\label{fig:comp}
\end{figure}

\subsubsection{Calcul d'erreur pour d=2}
\begin{verbatim}
d=2
Taux d'erreur sur l'ensemble d'entrainement: 24.07%
Taux d'erreur sur l'ensemble de validation: 19.05%	
\end{verbatim}

\subsubsection{Calcul d'erreur pour d=4}

\begin{verbatim}
d=4
Taux d'erreur sur l'ensemble d'entrainement: 4.63%
Taux d'erreur sur l'ensemble de validation: 4.76%
\end{verbatim}

\subsection{Avec densités de Parzen gaussienne isotropique}

\subsubsection{Classifieur de Parzen : sigma trop petit}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/bayes_parzen_petit.png} 
	\centering
	\caption{Classifieur de Parzen : sigma trop petit}
	\label{fig:comp}
\end{figure}

\subsubsection{Classifieur de Parzen : sigma trop grand}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/bayes_parzen_grand.png} 
	\centering
	\caption{Classifieur de Parzen : sigma trop grand}
	\label{fig:comp}
\end{figure}

\subsubsection{Classifieur de Parzen : sigma adéquat}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/bayes_parzen_bon.png} 
	\centering
	\caption{Classifieur de Parzen : sigma adéquat}
	\label{fig:comp}
\end{figure}

\subsubsection{Courbe d'apprentissage 2D}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/bayes_parzen_courbe_apprentissage_2d.png} 
	\centering
	\caption{Courbe d'apprentissage 2D}
	\label{fig:comp}
\end{figure}

Meilleur valeur : $\sigma = 0.43$

\subsubsection{Courbe d'apprentissage 4D}
\begin{figure}[H]
	\includegraphics[width=12cm]{images/bayes_parzen_courbe_apprentissage4D.png} 
	\centering
	\caption{Courbe d'apprentissage 4D}
	\label{fig:comp}
\end{figure}

Meilleur valeur : $\sigma = 0.02$

\subsubsection{Meilleur choix}
Cette exécution avec cette division particulière montre que :
\begin{itemize}
	\item La classification en dimension 4 est meilleur que la classification en dimension 2,
	\item La classification avec un noyau de Parzen donne le même résultat en dimension 4 et un meilleur en dimension 2,
	\item En dimension 2, l'erreur de validation avec gaussienne diagonale est de 19.05 \% et de 14.29 \% pour Parzen,
	\item En dimension 4, l'erreur de validation avec gaussienne diagonale et Parzen valent tous les deux 4.76 \%,
	\item La meilleur valeur de $\sigma$ en dimension 4 est de 0.02,
	\item La meilleur valeur de $\sigma$ en dimension 2 est de 0.43.
\end{itemize}
\vspace{5mm}

On peut essayer de généraliser les résultats. Ce choix sera réalisé de manière empirique en testant plusieurs $seeds$ différents de manière a faire ressortir une "tendance général".

Ces exécutions différentes semblent montrer que :
\begin{itemize}
	\item La classification en dimension 4 est meilleur que la classification en dimension 2,
	\item La classification avec un noyau de Parzen semble donner de meilleurs résultats,
	\item La valeur de sigma "optimal" varie grandement en fonction des données.
	\item L'erreur de classification avec gaussienne diagonale semble varier entre 2 et 6 \% et entre 0 et 4 \% pour Parzen.
\end{itemize}



\end{document}
